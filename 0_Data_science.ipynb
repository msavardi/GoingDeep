{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BodFmdlMfBMS"
      },
      "source": [
        "# Data science for a day\n",
        "based on [Ferrari Alessandro's work](https://github.com/alessandroferrari/Data_Scientist_for_a_Day)\n",
        "\n",
        "\n",
        "The  codelab  will  guide  you  through  the  effective  application  of  a  really  powerful  classification algorithm, the Support Vectors Machine, a.k.a.  SVM. To make things more enganging, the lab session iscontextualized in a Kaggle competition, the London Data Science competition.  Have you ever heard about Kaggle?  It is the place where world class data scientists compete.  If you make it through until the end ofthe lab, you will be able to make your submission and to show up in our class leaderboard.\n",
        "\n",
        "You probably know many theorical stuffs about classifiers, and you may be really good in showing off a lotof acronymous such as SVM, KNN, ANN and so on, however, you may feel as I did when I first approachedthe application of a classification task.  You may think about which kind of classifier is better to apply foryour challenge, or you may like to try an SVM as the first try but without any clue on the type of kernelthat you have to choose, and ...  what about the parameters ?!?  \n",
        "The default ones do not look to work atall, right?  If you are curious about the conclusion of this story, stay in tune with the lab, you will get someanswers.The dataset is the original one from the kaggle website.  It is a synthetically generated dataset, I personally think that is a great playground for tuning and improve the effectiveness of classification solutions.\n",
        "\n",
        "Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTE9vIGxfBMT"
      },
      "source": [
        "# Few very brief general concepts\n",
        "\n",
        "Supervised Learning is a branch of machine learning that studies identification of complex functions by analysis of labeled or continuously valued data, that are also referred as training data.  The training dataare a series of examples; each example consists of a features vector and the corresponding label, in case a classification function has to be identified,  conversely if it is a regression problem,  instead of labels thereare  continuous  values.   \n",
        "Classification  is  the  task  to  predict  classes  to  which  certain  data  belong.   The classification  function,  also  referred  as  classifier  or  as  hypothesis,  is  the  result  of  the  supervised  learningalgorithm.  Conversely, regression problems have the aim to identify a regression function in order to predictcontinuous value functions.\n",
        "In this codelab, just classification algorithms will be taken in analysis.  The development of a classification algorithm is composed by two different phases, the learning phase, where the classification function is learnedfrom the training set, and the classification phase, where the learned classification function is used to inferthe labels for new features vectors received in input. There  are  several  different  classification  algorithms  available.   While  the  number  of  training  sample sincreases,  the  performance  of  those  algorithms  asynthotically  tend  to  be  equivalent.   However,  in  certain conditions, there are certain algorithms that are more suitable than others.  Some algorithms are faster than others, some others have better learning in conditions of low-dimensionality of the training set, while certain others can achieve better classification rates but they are effective only in conditions of high dimensionality of the training set\n",
        "\n",
        "## Support Vector Machines\n",
        "Support Vector Machine algorithm is a very powerful classification algorithm, that is widely used inindustrial and academical applications.  SVM is designed in a way that allows a fast and efficient integration with kernel-based techniques, so in certain conditions it may give a cleaner and faster way for learningcomplex nonlinear functions, compared to other state-of-the-art algorithms such as neural networks or lo-gistic regression.  The way how the SVM cost function is defined allows very efficient solutions as a convexoptimization problem; this can guarantee great performance in several different conditions.  When SVM is applied using kernel methods, it can be considered fast until when the training set dimensionality does not become too big (indicatively, the training set should be smaller than 100000 examples).  The achieved levelof optimization is a result coming from a long effort of the research community.  Still,  this optimizationsmay be insufficient in case of a really big training set dimensionality, in such cases you may evaluate to usea neural network. SVM may be used as a linear classifier, if used without kernel methods, however in this lab session it willbe used to learn a nonlinear decision boundary, so it will be applied with kernel methods.  In general, if a sufficient number of training samples is available and there is not a priori knowledge about the characteristics of the hypothesis function that has to be learnt, it is suggested to apply SVM with radial basis functionskernel, also known as gaussian kernel.  Unless it is known that the samples belonging to different classes are linearly separable, or unless the number of training samples is very low, there are not many reasons thatsuggest to use SVM without kernels.  Today in the lab, SVM with gaussian kernels has been adopted.  Thegaussian kernel requires as a parameter the sigma of the ”gaussian similarity function”, it will be seen lateron how the selection of this parameter is important in order to have good classification results.  SVM hasbeen chosen between all the different algorithms because it is expected to be easier to train, faster in theconvergence  of  the  learning  phase,  faster  in  the  classification  phase,  flexible  to  the  different  distributionsthat may occur in the features space and it does not risk to get stuck in local minimum in the optimizationproblem like it may happens for some other algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNnWw6M4fBMT"
      },
      "source": [
        "## High Bias vs High Variance\n",
        "As anticipated beforehand, the SVM algorithm with gaussian kernels has an important parameter that has to be set, that is the sigma of the gaussian kernels. Smaller values of sigma corresponds to narrower kernel basis functions, that allows the algorithm to learn more complex wiggled curves. Conversely the bigger the values of sigma, the more regular and the simpler will be the learnt hypothesis.\n",
        "\n",
        "One may think that is always better to have an algorithm able to learn an as much complicated as possible hypothesis in order to achieve the best performances. This is indeed wrong, since sometimes the hypothesis that we have to learn are just regular. If it is attempted to fit a very complex model to a set of data that requires just a pretty regular decision boundary, in absence of a \"sufficiently huge\" amount of data, the hypothesis will probably suffer from overfitting or high variance.\n",
        "![Figure 1](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/template_quadratic_region_overfitting.jpg?raw=1)\n",
        "In this case it will be obtained a really wiggly decision boundary that will fit perfectly the training set and will guarantee very low training error, however it will generalize badly on new examples. As it is possible to see in Fig.1, the data are placed in a 2D features space in a way that suggests to separate them by a quadratic decision boundary, however since the data have been trained using an SVM with a very small sigma value, the decision boundary obtained is way more complicated and it is expected that will perform badly in the classification of new examples placed close to the boundary region.\n",
        "![Figure 2](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/template_quadratic_region_just_right.jpg?raw=1)\n",
        "In Fig.2 it is shown how the data are fitted with a better choice of the sigma parameter, the\n",
        "decision boundary is regular, it fits well the data and it is expected that the designed hypothesis will generalize well on new examples. So the choice of sigma is just fine.\n",
        "![Figure 3](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/template_quadratic_region_underfitting.jpg?raw=1)\n",
        "Conversely, in Fig.3 it is shown the opposite situation that may arise when fitting the data with a sigma parameters that is too big. The model that has been fitted to the data is not just a good model in order to separate the data. In fact the model is too regular and it will not be able to adapt to the dynamics of the training examples. This situation is called underfitting or high bias.\n",
        "\n",
        "In order to fix overfitting, the first thing to do is to verify to have chosen a good value of the sigma parameter by means of model selection, to fit a good model to the data. If it has been done a good choice and still overfitting persists, the number of features should be reduced and regularization techniques should be adopted. If overfitting persists even after that all these strategies have been tried out, it is suggested to collect more data. In fact, the bigger the number of training samples, the more the overfitting is unlikely to occur in overfitting problems. Conversely, if underfitting problem occurs, it has to be addressed by trying to fit a more complex model to the training set, reducing the sigma parameter. The second operation to do in order to remove underfitting is to increase the number of features by finding out new and more discriminative features for describe data.\n",
        "\n",
        "\n",
        "## Stop chatting about classification... Let us classify!\n",
        "In the learning phase of a classification algorithm, the hypothesis function that minimizes a certain cost function, that may vary depending on the algorithm that is selected, is searched. So, there is a search of the global minimum of the cost function relatively to that features space.  It is reasonable to assume that if features have ranges of different order of magnitude, the resulting features space will be really skewed and this may lead to several undesired counter effects. In order to avoid that the search of the global minimum follows complicated and slow paths, it is important to perform features scaling, all the features need to have reasonably similar ranges, in order to make more regular the features space. In fact, optimization operations work better in a smoother and more regular features space, and features scaling may help in achieving so.\n",
        "\n",
        "### Hypothesis evaluation\n",
        "In the previous section, the hypothesis has been evaluated by means of observations at plots of a 2D features space. To look at plots is a very powerful and intuitive tools in order to understand how good a certain hypothesis is. However, to plot features spaces is not always possible, since they may assume very high-dimensionality. So, more general methods have been developed in order to evaluate how good a certain learnt hypothesis is. As it has already been anticipated, the training error, that is a measure of how well the hypothesis fits to the training set, is not a good way to evaluate if it has been obtained a good decision margin, since it may happen to have overfitting, so even if the training error is really low, the obtained hypothesis will generalize really bad to new examples.\n",
        "\n",
        "So, it is suggested to split the labeled dataset in two different subsets. The first subset will be used as a training set (70{\\%} of the examples) while the second subset will used as a test set (30% of the examples). The test set will be used to evaluate how the hypothesis, that has been designed by means of supervised learning on the training set, will generalize on new examples. In order to objectively evaluate performances, a measure of the classification performances has been defined, that is called the misclassification error measure.\n",
        "\n",
        "Misclassification measure can be computed because test examples are labeled with their correct classes, so that is possible to compare label inferred by the classifier with ground truth labels. For each classified example in the test set, if it has been classified correctly, 0 will be added to the error measure, while if it has been classified incorrectly,  will be added $1/m$ , with *m* that is the total number of examples in the test set:\n",
        "\\begin{equation}\n",
        "Err = \\frac{1}{m_test}\\sum_{i=1}^{m_test}incorrect( x_{i} )\n",
        "\\end{equation}\n",
        "where *incorrect* is a function that returns 1 if the example $x_{i}$ has been incorrectly classified,  it returns 0 otherwise.\n",
        "This gives a measure of how well the learnt hypothesis will generalize on new examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3rs6qqbfBMT"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 1</b>:\n",
        "\n",
        "Below you can find an incomplete implementation of the concepts explained in this lab track. You are supposed to understand the concepts and complete the code by your own. Once you have completed the code, for performing the classification of the dataset you have to run the following cells in which are implemented some machine learning routines. In the first exercise you are supposed to implement the function ```misclassification_errors```. Read carefully the comments that you find there and follows the indications that you received from the subsection above.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:05.179576Z",
          "start_time": "2020-11-25T15:46:01.861425Z"
        },
        "id": "-ach6XOrfBMU"
      },
      "outputs": [],
      "source": [
        "!pip install nose\n",
        "!pip install scikit-learn\n",
        "!git clone https://github.com/Metunibs/Data_Scientist_for_a_Day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:05.192929Z",
          "start_time": "2020-11-25T15:46:05.182775Z"
        },
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-749eb51d74d0ca78",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "nc3M2bYwfBMU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import os\n",
        "import numpy as np\n",
        "os.chdir('Data_Scientist_for_a_Day')\n",
        "\n",
        "\n",
        "def dataset_scaling(X):\n",
        "\n",
        "    scaler=preprocessing.StandardScaler().fit(X)\n",
        "    X=scaler.transform(X)\n",
        "\n",
        "    return X , scaler\n",
        "\n",
        "\n",
        "def misclassification_errors(classifier, X_tr, y_tr, X_cv, y_cv):\n",
        "    \"\"\"\n",
        "    TODO: Exercise 1\n",
        "    Given an already trained classifier, the training set features and labels, and the\n",
        "    cross-validation features and labels, compute the misclassification error measure\n",
        "    on the training set and on the cross validation set, as explained in the lab track.\n",
        "    Try to do it without for loops. That does not mean that you can use while loop instead.\n",
        "    \"\"\"\n",
        "    raise Exception(\"Implement your own misclassification error measure!\")\n",
        "\n",
        "\n",
        "\n",
        "    return tr_err, cv_err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:05.203839Z",
          "start_time": "2020-11-25T15:46:05.194869Z"
        },
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-be09ac4887af4eb9",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false
        },
        "id": "cirkEhmBfBMV"
      },
      "outputs": [],
      "source": [
        "from nose.tools import assert_equal, assert_true\n",
        "# Check only consistency\n",
        "cl = SVC()\n",
        "X_tr, X_cv, y_tr, y_cv = train_test_split(np.random.randn(100,10), np.random.randint(0,2,100))\n",
        "cl.fit(X_tr, y_tr)\n",
        "res = misclassification_errors(cl, X_tr, y_tr, X_cv, y_cv)\n",
        "\n",
        "assert_equal(len(res), 2)\n",
        "assert_true(isinstance(res[0], float))\n",
        "assert_true(isinstance(res[1], float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NyS6-qtfBMV"
      },
      "source": [
        "## Error metrics for skewed classes\n",
        "Consider for instance that the problem of cancer classification is taken in analysis. Suppose that for each patients some features are extracted and these features well represent the clinical situation of the patient, so it is reasonable to think that will be possible to infer if the patient has the cancer by looking at them. After that learning has been done, it is obtained an algorithm that classifies correctly the 99\\% of the examples in the test set. This sounds like a pretty remarkable achievement, however what was unsaid, it is that only the 1\\% of the population have cancer. So, 99\\% is the error rate that can be obtained by a dumb-classifier that predicts always that the patient has not cancer. The obtained algorithm does not look attractive anymore.\n",
        "\n",
        "In this case, it is said that there are skewed classes, this is common when positive samples occur very rarely. When we are analyzing such skewed classes is necessary to come up with different evaluation metrics. In particular, precision and recall metrics will be adopted. Let us us first present few definitions. Given a binary classification problem, it is defined as a true positive an example that is actually a positive and it is classified as a positive. It is defined as a true negative an example that is actually a negative and it is correctly classified as a negative. Then, it is defined as a false positive an example which its actual class is negative, but it is misclassified as a positive. Conversely, it is defined as a false negative an example that is actually positive and it is misclassified as negative.\n",
        "\n",
        "After that this bit notation has been introduced, it is possible to define the precision as following:\n",
        "\\begin{equation}\n",
        "Precision= \\frac{True positives}{True positives + False Positives}\n",
        "\\end{equation}\n",
        "Recalling the previous example of cancer prediction, in case it is obtained a classifier with really high precision, if it is predicted that the patient has the cancer, there is a high probability that the patient actually has the cancer. So precision answers at the question: of all the patients that were predicted to have cancer, what fraction actually has the cancer?\n",
        "\n",
        "Now, another metric will be introduced in order to answer at the other question: of all patients that actually have cancer, what fraction did we correctly detect as having cancer? This metric is called recall and it is defined as following:\n",
        "\\begin{equation}\n",
        "Recall = \\frac{True positives}{True positives + False negatives}\n",
        "\\end{equation}\n",
        "If it is obtained a classification algorithm with high recall, it is highly probable that the\n",
        "cancer will be correctly detected to almost all the patients that actually have cancer.\n",
        "\n",
        "Finally, we define a metric that attempt to give a measure of how good is our model in maximizing both precision and recall:\n",
        "\\begin{equation}\n",
        "f1\\_score = 2 * \\frac{precision*recall}{precision+recall}\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL5lV1CGfBMV"
      },
      "source": [
        "## Model Selection\n",
        "In order to fit the correct model to the data, for avoiding overfitting and underfitting, it is important to select the best parameters. However, it has not yet been explained how to correctly select these parameters. In fact, the typical machine learning challenge does not have enough *a priori* knowledge about the characteristics of the model that should be fitted. So, it has been necessary to develop a series of techniques in order to select the most suitable model, given the data.\n",
        "\n",
        "In the SVM classification algorithm with gaussian kernels, the parameters to set are the sigma of the gaussian kernel and the regularization constant. Regularization determines the regularity or smoothness of the obtained decision margin. The regularization constant is usually referred as C and it controls the strenght of regularization during the optimization process. It is a terrific tool to address overfitting.\n",
        "\n",
        "Now a question arises on how a good sigma parameter can be selected, so on how it can be selected the best model to fit the data. A set of possible different sigma and C values to evaluate is defined. So, let us say that is expected that the sigma parameters to be evaluated will be between the order of $10^{-7}$ and $10^{7}$ ; a set that starts with an element of value $10^{-7}$ and with the next elements that increase at logarithmic steps, until it is reached $10^{-7}$, such as {$10^{-7}$, $10^{-6}$, $10^{-5}$, $10^{-4}$,....,$10^{7}$}, is defined. The same applies for the regularization constant C, that will take value from a set with the same items as the sigma parameter. For each combination of these sigma and C parameters, the SVM training algorithm is applied on the training set.\n",
        "\n",
        "So, a hypothesis for each combination of the sigma and C parameters values is obtained. Then, the performances of all these hypothesis will be evaluated on the test set. The parameters corresponding to the hypothesis that will lead to the smallest classification error, will be selected as the best one. Once the best parameters are selected, to define new sets around the best values with higher resolution and to re-iterate model selection on the new sets can lead to a finer model selection.\n",
        "\n",
        "![Training error plot of the model selection](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/tr_err_graph.png?raw=1)\n",
        "![Cross-validation error plot of the model selection.](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/cv_err_graph.png?raw=1)\n",
        "![Accuracy plot of the model selection](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/acc_graph.png?raw=1)\n",
        "\n",
        "![Precision plot of the model selection](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/pr_graph.png?raw=1)\n",
        "![Recall plot of the model selection.](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/rec_graph.png?raw=1)\n",
        "![f1 plot of the model selection](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/f1_graph.png?raw=1)\n",
        "\n",
        "## Dataset partitions re-arrangement\n",
        "In order to explain the following technique an example will be introduced first. Suppose that is desired to classify whether an object is a car or not. In the dataset there are a series of images of utility cars, sport cars and other objects. It is pretty much a fact that a desirable sport car does not look as ugly as a blend unemotional utility car. The sport car will be shorter in height, it will be longer and it will have several appearance differences compared to the utility car. Now, suppose also that the number of images with a utility car is way greater than the number of images with a sport car, that are just the 1% of the objects in the dataset. It is desired that both utility cars and sport cars will be labeled as belonging to the category car. Let us say the dataset is partitioned in a training set with 70% of the samples and a test set with the 30% of the examples.\n",
        "\n",
        "Now, let us imagine that not even one of the images containing sport cars end up to be included in the training set, but they all end up in the test set. This will lead to a systematic error and all the sport cars in the test set will be classified as non-car objects, so it may be thought that the features chosen in order to classify sport cars are not good at all, however it may be a wrong conclusion, since there are not actual evidences that may measure how well the chosen features are in order to classify the sport cars. So, as it has been said before, sport cars may look fancier, thus it is not straightforward that the choice of features that will classify well utility cars, will have good performances on sport cars as well and suppose that is harder to have good classification performances on sport cars.\n",
        "\n",
        "Let us imagine the case where all the sport cars end up in the training set. The classification algorithm performs really well on utility cars and it is obtained a really high score in performances evaluation done on the test set. So, it will be expected that the designed algorithm will behave at the same way also on new examples. However, when new sport car examples will be classified, it may arise that the classifier is not effective on them since the features that have been chosen were insufficient, but this problem was not diagnosed in the algorithm debugging phase and so the right countermeasures have not been adopted in the algorithm design phase.\n",
        "\n",
        "In order to avoid to occur in a partitioning of the examples that leads to such unpleasant situations, several random partitions are extracted and for all of them the results of the classification algorithm are taken in analysis. If a sufficient number of partitions have been extracted, the worst results and the best results may be assumed as \"reliable\" boundaries of the performances that will be get when new examples will be examined. Moreover, by the gap between the worst and the best results, it is possible to understand how much our dataset suffers from the problems described above and it may be a hint that countermeasures should be adopted. In order to understand which countermeasures to adopt, is important to perform error analysis. Then by calculating the average of the results obtained on the different partitions, it is possible to understand whether the majority of the partitions give results that are closer to the better case or to the worst case. By averaging, a better performances evaluation is obtained.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 2</b>:\n",
        "\n",
        "Now, you have all the necessary elements to complete the model selection code of the lab. You are supposed to have understood the concept of cross-validation for selecting the best parameters and the concept of dataset partitions re-arrangement for having robust statistics. Now, you can write the missing part of code in the method ```C_gamma_selection``` of the ModelSelection class. First understand the existing code and read carefully the comments, then implement your own part.\n",
        "        \n",
        "Once you have finished the code of this exercise, look at the plots that you obtain from the model selection, which are the ```gamma``` and the ```C``` parameters that guarantee the best results?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:05.658339Z",
          "start_time": "2020-11-25T15:46:05.627606Z"
        },
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4db4fb8737693f34",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "aUQH2V6TfBMW"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "class ModelSelection(object):\n",
        "\n",
        "    def __init__(self, C_list = None, gamma_list = None):\n",
        "\n",
        "        assert C_list is None or isinstance(C_list, list)\n",
        "        assert gamma_list is None or isinstance(gamma_list,list)\n",
        "\n",
        "        if C_list is None:\n",
        "            #regularization parameters\n",
        "            self.C_list=[0.0000001,0.000001,0.00001,\n",
        "                    0.0001,0.001,0.01,0.1,1,10,\n",
        "                    100,1000,10000,100000,1000000]\n",
        "        else:\n",
        "            self.C_list = C_list\n",
        "\n",
        "        if gamma_list is None:\n",
        "            self.gamma_list = [0.0000001,0.000001,0.00001,\n",
        "                          0.0001,0.001,0.01,0.1,1,10,\n",
        "                          100,1000,10000,100000,1000000]\n",
        "        else:\n",
        "            self.gamma_list = gamma_list\n",
        "\n",
        "\n",
        "    def C_gamma_selection(self,X, y, C_list = None, gamma_list = None,\n",
        "                          classifier_by_C_function_params = None,\n",
        "                          test_size = 0.3, n_iterations = 20 ):\n",
        "\n",
        "        if C_list is not None:\n",
        "            self.C_list = C_list\n",
        "\n",
        "        if gamma_list is not None:\n",
        "            self.gamma_list = gamma_list\n",
        "\n",
        "        tr_err_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)), dtype=np.float)\n",
        "        cv_err_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)), dtype=np.float)\n",
        "\n",
        "        acc_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)),dtype=np.float)\n",
        "        prec_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)), dtype=np.float)\n",
        "        recall_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)), dtype=np.float)\n",
        "        f1_by_C_and_gamma=np.zeros((len(self.C_list),len(self.gamma_list)), dtype=np.float)\n",
        "\n",
        "        set_ripartitions = StratifiedShuffleSplit(n_splits=n_iterations,\n",
        "                                                  test_size = test_size)\n",
        "\n",
        "        n_iter=set_ripartitions.get_n_splits(X, y)\n",
        "\n",
        "        for train,test in tqdm(set_ripartitions.split(X, y)):\n",
        "            X_tr,X_cv,y_tr,y_cv =X[train],X[test],y[train],y[test]\n",
        "\n",
        "            index_C=0\n",
        "            for C in self.C_list:\n",
        "\n",
        "                idx_gamma=0\n",
        "                for gamma in self.gamma_list:\n",
        "                    \"\"\"\n",
        "                    TODO: Exercise 2\n",
        "                    For each combination of C and gamma, compute the training error, cross-validation error,\n",
        "                    accuracy, precision, recall and f1 score obtained with the relative SVM rbf classifier,\n",
        "                    obtained averaging the results obtained by the different dataset partitions re-arrangements.\n",
        "                    The results will be stored relatively in the numpy arrays tr_err_by_C_and_gamma,\n",
        "                    cv_err_by_C_and_gamma, acc_by_C_and_gamma, prec_by_C_and_gamma, recall_by_C_and_gamma,\n",
        "                    f1_score_by_C_and_gamma created previously. Columns contain the C index, while rows contain\n",
        "                    the gamma index. While doing the exercise, you may find useful the SVC class in sklearn.svm\n",
        "                    module, the misclassification_errors that you implemented in the previous exercise and the\n",
        "                    score functions that are implemented in sklearn metrics.\n",
        "                    \"\"\"\n",
        "\n",
        "                    raise Exception(\"Wake up! You are supposed to implement this part of code!\")\n",
        "\n",
        "\n",
        "                    idx_gamma=idx_gamma + 1\n",
        "\n",
        "                index_C=index_C + 1\n",
        "\n",
        "\n",
        "        result=dict()\n",
        "        result[\"C_list\"]=self.C_list\n",
        "        result[\"gamma_list\"]=self.gamma_list\n",
        "        result[\"tr_err_by_C_and_gamma\"]=tr_err_by_C_and_gamma\n",
        "        result[\"cv_err_by_C_and_gamma\"]=cv_err_by_C_and_gamma\n",
        "        result[\"acc_by_C_and_gamma\"]=acc_by_C_and_gamma\n",
        "        result[\"prec_by_C_and_gamma\"]=prec_by_C_and_gamma\n",
        "        result[\"recall_by_C_and_gamma\"]=recall_by_C_and_gamma\n",
        "        result[\"f1_by_C_and_gamma\"]=f1_by_C_and_gamma\n",
        "\n",
        "        return result\n",
        "\n",
        "class SVM(object):\n",
        "\n",
        "    def model_selection(self,X,y, C_list = None, gamma_list = None, test_size = 0.3, n_iterations = 20):\n",
        "\n",
        "        assert X.any()!=None and y.any()!=None\n",
        "        assert len(X)==len(y)\n",
        "\n",
        "        model_selection = ModelSelection(C_list=C_list,gamma_list=gamma_list)\n",
        "        parameters_result = model_selection.C_gamma_selection(X, y,\n",
        "                                                              test_size = 0.3, n_iterations = n_iterations)\n",
        "\n",
        "        return parameters_result\n",
        "\n",
        "\n",
        "    def training(self,X,y,C=1,gamma=None):\n",
        "\n",
        "        assert isinstance(C,(int,float))\n",
        "        assert isinstance(gamma,(int,float))\n",
        "\n",
        "        self.classifier = SVC(kernel=\"rbf\",C=C,gamma=gamma)\n",
        "        self.classifier.fit(X,y)\n",
        "\n",
        "        return self.classifier\n",
        "\n",
        "\n",
        "    def classify(self,X):\n",
        "\n",
        "        assert hasattr(self,\"classifier\")\n",
        "\n",
        "        return self.classifier.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:28.674511Z",
          "start_time": "2020-11-25T15:46:06.851407Z"
        },
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-6002ee06c61d3b60",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false
        },
        "scrolled": true,
        "id": "1oan64lDfBMX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from utilities import plot_3d, save_csv_submitted_labels, plot_learning_curves\n",
        "%matplotlib inline\n",
        "\n",
        "testdir = os.getcwd()\n",
        "\n",
        "print (\"Reading the dataset from file...\")\n",
        "# Read data\n",
        "train = np.genfromtxt(open(os.path.join(testdir, 'data/train.csv'),'rb'), delimiter=',')\n",
        "target = np.genfromtxt(open(os.path.join(testdir, 'data/trainLabels.csv'),'rb'), delimiter=',')\n",
        "test = np.genfromtxt(open(os.path.join(testdir, 'data/test.csv'),'rb'), delimiter=',')\n",
        "print (\"Dataset loaded!\")\n",
        "#features scaling\n",
        "print (\"Starting features preprocessing ...\")\n",
        "\n",
        "dataset_scaled, scaler = dataset_scaling(np.vstack((train,test)))\n",
        "train_scaled = dataset_scaled[:1000]\n",
        "test_scaled = dataset_scaled[1000:]\n",
        "\n",
        "print (\"Features preprocessing done!\")\n",
        "\n",
        "classification_obj=SVM()\n",
        "\n",
        "print (\"Starting model selection ...\")\n",
        "\n",
        "#performing model selection\n",
        "\n",
        "C_list = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\n",
        "gamma_list = [0.0001,0.001,0.01,0.1,1,10, 100,10000]\n",
        "\n",
        "ms_result = classification_obj.model_selection(train_scaled,target,n_iterations=3,\n",
        "                                               C_list=C_list,\n",
        "                                               gamma_list=gamma_list)\n",
        "\n",
        "#displaying model selection\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"acc_by_C_and_gamma\"], zlabel=\"accuracy\", title=\"Accuracy by C and gamma\")\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"recall_by_C_and_gamma\"], zlabel=\"recall\", title=\"Recall by C and gamma\")\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"prec_by_C_and_gamma\"], zlabel=\"precision\", title=\"Precision by C and gamma\")\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"f1_by_C_and_gamma\"], zlabel=\"accuracy\", title=\"f1 score by C and gamma\")\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"tr_err_by_C_and_gamma\"], zlabel=\"training error\", title=\"Training error score by C and gamma\")\n",
        "plot_3d(x=ms_result[\"gamma_list\"], y=ms_result[\"C_list\"], z=ms_result[\"cv_err_by_C_and_gamma\"], zlabel=\"cross-validation error\", title=\"Cross-validation error score by C and gamma\")\n",
        "plt.show()\n",
        "\n",
        "#entering the C and gamma chosen\n",
        "print (\"Plotted graphics for model selection. Choose the best C and gamma ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:46:46.339308Z",
          "start_time": "2020-11-25T15:46:28.676568Z"
        },
        "id": "at8OgsBefBMX"
      },
      "outputs": [],
      "source": [
        "while True:\n",
        "    C_str = input(\"Enter the C value suggested by model selection:\")\n",
        "    try:\n",
        "        C = float(C_str)\n",
        "    except Exception as e:\n",
        "        print (\"Invalid C inserted. C has to be numeric. Exception: {0}\".format(e))\n",
        "        continue\n",
        "    break\n",
        "\n",
        "while True:\n",
        "    gamma_str = input(\"Enter the gamma value suggested by model selection:\")\n",
        "    try:\n",
        "        gamma = float(gamma_str)\n",
        "    except Exception as e:\n",
        "        print (\"Invalid gamma inserted. gamma has to be numeric. Exception: {0}\".format(e))\n",
        "        continue\n",
        "    break\n",
        "\n",
        "print (\"Parameters selection performed! C = {0}, gamma = {1}\".format(C, gamma))\n",
        "\n",
        "#training\n",
        "print (\"Performing training...\")\n",
        "\n",
        "classifier = classification_obj.training(train_scaled, target, C=C, gamma=gamma)\n",
        "\n",
        "print (\"Training performed!\")\n",
        "\n",
        "#prediction on kaggle test set\n",
        "print (\"Performing classification on the test set...\")\n",
        "\n",
        "predicted = classification_obj.classify(test_scaled)\n",
        "\n",
        "print (\"Classification performed on the test set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYgSTxDsfBMX"
      },
      "source": [
        "## Submit your result!\n",
        "\n",
        "- Register [here](https://promoted-oriented-gnu.ngrok-free.app/register)\n",
        "- Put your user and password in the call below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:47:53.573731Z",
          "start_time": "2020-11-25T15:47:52.375175Z"
        },
        "id": "wvODvHQffBMX"
      },
      "outputs": [],
      "source": [
        "from utilities import send_results\n",
        "from IPython.display import FileLink\n",
        "from IPython.display import HTML\n",
        "\n",
        "filename='out.csv'\n",
        "res = send_results(predicted, filename=filename, username='YOUR-USER', password='YOUR-PASSWORD',\n",
        "                   email='YOUR-EMAIL@unibs.it', URL='https://promoted-oriented-gnu.ngrok-free.app/api_upload')\n",
        "HTML(res.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-11-25T15:48:07.105624Z",
          "start_time": "2020-11-25T15:48:07.099040Z"
        },
        "id": "C0hxm1s3fBMX"
      },
      "outputs": [],
      "source": [
        "FileLink(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2exF2dn6fBMY"
      },
      "source": [
        "## Learning Curves\n",
        "Learning curves are often useful tools to diagnose whether a learning algorithm may be suffering from overfitting, underfitting or a bit of both. Beforehand let us introduce a bit  of notation. The  training error $J_{train}$ is the misclassification error measure applied to the training set, while the cross-validation error $J_{cv}$ is the misclassification error measure applied to the test set. In formulas:\n",
        "\\begin{equation}\n",
        "J_{train} = \\frac{1}{2m_{tr}} \\sum_{i=1}^{m_{tr}}incorrect( x_i )\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "J_{cv} = \\frac{1}{2m_{cv}} \\sum_{j=1}^{m_{cv}}incorrect( x_j )\n",
        "\\end{equation}\n",
        "where $m_{tr}$ is the number of samples in the training set, $x^i$ is the features vector of the i-th example in the training set, $m_{cv}$ is the number of samples in the test set, $x_j$ is the features vector of the j-th example in the test set and incorrect is a function that gives 1 whether the example is misclassified, 0 whether is correctly classified.\n",
        "\n",
        "In order to plot a learning curve, $J_{train}$ and $J_{cv}$ have to be draw as a function of the number of training examples. What has just been stated may result confusing, in fact the number of examples in a training set is a constant value. What was meant is to perform learning on a reduced training set, for instance starting from a 10 examples subset of the training set, calculate $J_{train}$ of the classifier obtained from the reduced training set and $J_{cv}$ on the complete test set, then iterate these steps while increasing the number of examples in the reduced training set, until the reduced training set corresponds to the entire training set.\n",
        "\n",
        "When the training set is really small, like 10 examples, it is expected to have a really low\n",
        "training error since the hypothesis can easily overfit the data, conversely it is expected a\n",
        "really high cross-validation error, since very few samples have been encountered and so the\n",
        "algorithm will not generalize well for new samples. As the number of samples in the\n",
        "training set increases, the training error increases, since it becomes harder that the\n",
        "hypothesis manages to fit all the training examples, while the cross-validation error\n",
        "decreases, since it is reduced the overfitting and the decision region is better defined.\n",
        "\n",
        "When the learning process has converged to an optimal decision boundary, it is expected that the training error and the cross-validation error converge to the same reasonably small error. In fact it is possible to empirically observe that the learning curves tend to flatten out as the number of examples increases.\n",
        "\n",
        "When an algorithm suffers from high bias, it is expected to have a high training error and a cross-validation error equivalent to the training error. In fact, with high bias, if new samples will be added, it is not expected that the decision boundary will change that much. If it is taken as example the case of underfitting shown in Fig.6, once the \"best\" possible straight line has been found (and it will be found pretty fast), adding new training examples will not change the decision boundary that will tend to stabilize, consequently the performances of the classifier will not change as well. The training error will be high and it will get constant relatively soon, since the decision boundary will stabilize relatively fast. For the same reason, if m is sufficiently large, the cross-validation error will be nearly equivalent to the training error, since the decision boundary will fit both sets at the same bad way. So, the learning curves of an algorithm suffering from high bias flatten out relatively soon and they flatten out at the same error value, as shown in Figure below.\n",
        "\n",
        "### Learning curves of an algorithm suffering of high bias.\n",
        "![Learning curves of an algorithm suffering of high bias.](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/learning_curves_high_bias.jpg?raw=1)\n",
        "\n",
        "\n",
        "Now the case when a learning algorithm suffers from high variance will be taken in analysis. In that case, it is expected that it relatively fits well the data and consequently the error on the training set will be small compared to the error obtained when the hypothesis generalizes to new examples. Thus, the learning curves of an algorithm in overfitting will have between $J_{train}$ and $J_{cv}$ the gap shown in Figure below, that can be considered the diagnostic evidence of overfitting. By adding more data examples to the dataset is expected to reduce the gap and so to increase the performances of the learning algorithm.\n",
        "\n",
        "### Learning curves of an algorithm suffering of high variance.\n",
        "![Learning curves of an algorithm suffering of high variance.](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/learning_curves_high_variance.jpg?raw=1)\n",
        "### Learning curves of an algorithm with good convergence.\n",
        "![Learning curves of an algorithm with good convergence.](https://github.com/Metunibs/Data_Scientist_for_a_Day/blob/master/img/learning_curves_just_fine.jpg?raw=1)\n",
        "\n",
        "It is important to take into account that if a learning algorithm suffers from high bias, it is just a waste of time to collect more data, while in the case of overfitting, after that all the others tricks have been tried out, collect more data can actually improve performances.\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<b>EXERCISE 3</b>:\n",
        " <ul>\n",
        "\n",
        "Finally, we are arrived at the last exercise. This exercise, require that you have understood the concept of learning curves. Almost all the code necessary to implement learning curves have already been provided, however you need to understand it and you have to complete the last part. Read through carefully the code of the compute and StratifiedShuffleMask methods of the LearningCurves class. First, understand the code of these methods, by using also the documentation of numpy and scikit-learn. Then, implement the missing part of the code following the indications in the comments.\n",
        "        \n",
        "After that the code is complete, you can run all the code until the plots of the learning curves. What do the learning curves suggest us? Is it suffering of underfitting, overfitting, or do you think it is fine?\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0bbc8a1b715577a1",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "eLGITJUmfBMY"
      },
      "outputs": [],
      "source": [
        "class LearningCurves(object):\n",
        "\n",
        "    def stratifiedShuffleMask(self, y,m):\n",
        "        pos_m=np.ceil(m/2)\n",
        "        neg_m=m-pos_m\n",
        "\n",
        "        max_pos=np.sum(y)\n",
        "        max_neg=len(y)-max_pos\n",
        "\n",
        "        if(pos_m>max_pos):\n",
        "            pos_m=max_pos\n",
        "            neg_m=m-pos_m\n",
        "\n",
        "        if(neg_m>max_neg):\n",
        "            neg_m=max_neg\n",
        "            pos_m=m-neg_m\n",
        "\n",
        "        mask=np.zeros(len(y),dtype=np.float)\n",
        "\n",
        "        idx=0\n",
        "        while pos_m>0:\n",
        "            if y[idx]==1:\n",
        "                mask[idx]=1\n",
        "                pos_m-=1\n",
        "            idx+=1\n",
        "\n",
        "        idx=0\n",
        "        while neg_m>0:\n",
        "            if y[idx]==0:\n",
        "                mask[idx]=1\n",
        "                neg_m-=1\n",
        "            idx+=1\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def compute(self,X,y,C,gamma, test_size=0.3, n_iterations = 5, training_set_minsize = 10, learning_curves_step = 20):\n",
        "\n",
        "        assert len(X)==len(y)\n",
        "        assert len(y)>training_set_minsize\n",
        "\n",
        "        assert isinstance(C, (int, float))\n",
        "        assert isinstance(gamma, (int, float))\n",
        "\n",
        "        train_size=int( round( (1-test_size) * len(y) ))\n",
        "\n",
        "        set_ripartitions = StratifiedShuffleSplit(n_splits=n_iterations,\n",
        "                                                  test_size = test_size)\n",
        "\n",
        "        n_iter=set_ripartitions.get_n_splits(X, y)\n",
        "\n",
        "        n_samples=X.shape[0]\n",
        "        n_features=X.shape[1]\n",
        "\n",
        "        m_list=range(training_set_minsize,train_size,learning_curves_step)\n",
        "\n",
        "        tr_errors=np.zeros((len(m_list),1),dtype=np.float)\n",
        "        cv_errors=np.zeros((len(m_list),1),dtype=np.float)\n",
        "\n",
        "        for train,test in set_ripartitions.split(X, y):\n",
        "            X_tr,X_cv,y_tr,y_cv =X[train],X[test],y[train],y[test]\n",
        "\n",
        "            idx=0\n",
        "            for m in m_list:\n",
        "\n",
        "                y_mask = self.stratifiedShuffleMask(y_tr,m)\n",
        "                x_mask = np.kron(np.ones((n_features,1)),y_mask).T\n",
        "\n",
        "                reduced_X = X_tr[x_mask!=0].reshape(m,n_features)\n",
        "                reduced_y = y_tr[y_mask!=0]\n",
        "\n",
        "\n",
        "                \"\"\"\n",
        "                TODO: Exercise 3\n",
        "                Read the code of the current method \"compute\" and understand what is\n",
        "                happening. Once you have understood the code, try to understand the\n",
        "                meaning of the stratifiedShuffleMask method. What is that method suppose\n",
        "                to do? What do reduced_X and reduced_y contain?\n",
        "                Then, compute for each m, the training error and the cross-validation error\n",
        "                averaged by the different re-arranged dataset ripartitions, and store them\n",
        "                relatively in the tr_errors and cv_errors numpy vectors, order by the idx index.\n",
        "                \"\"\"\n",
        "\n",
        "                raise Exception(\"One last effort! It is the last exercise.\")\n",
        "\n",
        "\n",
        "                idx+=1\n",
        "\n",
        "                result=dict()\n",
        "                result[\"m_list\"]=m_list\n",
        "                result[\"tr_errors\"]=tr_errors\n",
        "                result[\"cv_errors\"]=cv_errors\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-4567a2eda929841c",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false
        },
        "id": "3Skjd_rHfBMY"
      },
      "outputs": [],
      "source": [
        "#plot learning curves\n",
        "print (\"Plotting learning curves...\")\n",
        "learning_curves = LearningCurves()\n",
        "learning_curves_result = learning_curves.compute(X=train_scaled,y=target,C=C,gamma=gamma)\n",
        "\n",
        "plot_learning_curves(x1=learning_curves_result[\"m_list\"],\n",
        "                     y1=learning_curves_result[\"tr_errors\"],\n",
        "                     x2=learning_curves_result[\"m_list\"],\n",
        "                     y2=learning_curves_result[\"cv_errors\"])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print (\"Congratulations! You have finished this codelab, you rock!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6CtnOpqfBMY"
      },
      "source": [
        "# Conclusion\n",
        "If you have got through all the lab and you have understood all the concepts, you have reached a good starting level for applying machine learning tools. For improving your machine learner skills, I suggest you to follow the Machine Learning course of the Prof. Andrew Ng at www.coursera.org. In the scikit-learn website, you can find really interesting tutorials, that can be useful for improve your practical skills."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2n8jHmCfBMZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}